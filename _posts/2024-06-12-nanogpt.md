---
layout: post
author: bookstall
tags: LLM, GPT-2
categories: [LLM, GPT-2]
excerpt: NanoGPT æŠ€æœ¯è§£æ
keywords: LLM, GPT-2
title: ä»é›¶å¼€å§‹æ„å»º NanoGPT
mathjax: true
---


## æ•°æ®é›†

### FineWeb æ•°æ®é›†

https://huggingface.co/datasets/HuggingFaceFW/fineweb

![FineWeb æ•°æ®é›†](https://huggingface.co/datasets/HuggingFaceFW/admin/resolve/main/fineweb-logo.png)

How to download and use ğŸ· FineWeb
You can load the full dataset or a specific crawl/dump (see table below). Dumps have the format `CC-MAIN-(year)-(week number)`.

(Smaller) sample versions
Along with config default (all the data), and the configs for each individual dump, you can also download the following configs:

- `sample-350BT`: a subset randomly sampled from the whole dataset of around 350B gpt2 tokens (388GB)

- `sample-100BT`: a subset randomly sampled from the whole dataset of around 100B gpt2 tokens (277.4GB)

- `sample-10BT`: a subset randomly sampled from the whole dataset of around 10B gpt2 tokens (27.6GB)

sample-10B was sampled from sample-100B which in turn was sampled from sample-350BT.


### FineWeb-EDU æ•°æ®é›†


### æ•°æ®é›†é¢„å¤„ç†

ä¸‹è½½ FineWeb-EDU æ•°æ®é›†ï¼Œå¹¶å°†æ¯ä¸€ä¸ª token ids å­˜å‚¨æˆ 100 ä¸ª npy æ–‡ä»¶ï¼Œä»¥ä¾¿åç»­è®­ç»ƒå’ŒéªŒè¯é˜¶æ®µåŠ è½½ä½¿ç”¨ã€‚

```shell
$ HF_ENDPOINT=https://hf-mirror.com python fineweb.py
```

è¾“å‡ºç»“æœï¼š

```shell
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1571/1571 [00:05<00:00, 301.63it/s]
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 541M/541M [03:12<00:00, 2.80MB/s]
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.15G/2.15G [04:25<00:00, 8.10MB/s]
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.15G/2.15G [19:52<00:00, 1.80MB/s]
Generating train split: 9672101 examples [02:00, 80527.00 examples/s] | 461M/2.15G [04:24<20:11, 1.39MB/s]
Loading dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 647.87it/s]
Shard 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000000/100000000 [00:12<00:00, 7945080.25tokens/s]
Shard 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99997871/100000000 [00:12<00:00, 7973708.84tokens/s]
Shard 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99999982/100000000 [00:13<00:00, 7689462.98tokens/s]

Shard 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99999499/100000000 [00:13<00:00, 7628157.79tokens/s]
Shard 99:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 53989101/100000000 [00:08<00:07, 6468649.01tokens/s]
```

## è®­ç»ƒï¼šä½¿ç”¨ FineWeb-EDU æ•°æ®é›†

```shell
$ torchrun --standalone --nproc_per_node=2 train_gpt2.py

$ OMP_NUM_THREADS=8 nohup torchrun --standalone --nproc_per_node=2 train_gpt2.py > train_gpt2_64_1024_node2.log 2>&1 &
```

## ä¸€äº›é”™è¯¯

ç”±äºåœ¨ Nvidia V100S ä¸Šè¿è¡Œï¼Œå¯¼è‡´åŸå…ˆçš„ä»£ç ä¼šæœ‰ä¸€å®šçš„é—®é¢˜ã€‚å› æ­¤ï¼Œç‰¹æ„è¿›è¡Œè®°å½•ã€‚

æ³¨æ„ï¼šåœ¨ Google Colab ä¸Šçš„ T4 GPU è¿è¡Œæ—¶ï¼ŒåŸå…ˆçš„ä»£ç ä¸å­˜åœ¨é—®é¢˜ã€‚

### numpy uint16 to torch long 

ç›´æ¥å°† uint16 çš„ numpy æ•°æ®è½¬ä¸º PyTorch Longï¼š

```python
ptt = torch.tensor(npt, dtype=torch.long)
```

æŠ¥é”™ï¼š

```shell
TypeError: can't convert np.ndarray of type numpy.uint16. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.
```

### Fused AdamW

ç”±äºæœåŠ¡å™¨æœ€å¼€å§‹çš„ torch ç‰ˆæœ¬è¾ƒè€ï¼Œä¸æ”¯æŒ fused å‚æ•°ã€‚å› æ­¤åœ¨ä½¿ç”¨ Fused AdamW æ—¶å‡ºç°æŠ¥é”™ï¼š

```shell
torch                   1.13.1
torchelastic            0.2.2
torchtext               0.14.1
torchvision             0.14.1

TypeError: AdamW.__init__() got an unexpected keyword argument 'fused'
    optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)
  File "/root/code/build-nanogpt/train_gpt2.py", line 204, in configure_optimizers
    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)
```

ä½¿ç”¨ `pip install --upgrade torch` å‡çº§åˆ°æœ€æ–°çš„ PyTorchï¼ˆ2.3.1ï¼‰ï¼š

```shell
torch                    2.3.1
torchelastic             0.2.2
torchtext                0.14.1
torchvision              0.14.1
```

### with torch.autocast()

Bfloat16 on nvidia V100 gpuï¼ˆä¸æ”¯æŒï¼‰

```shell
[rank0]:   File "/root/code/build-nanogpt/train_gpt2.py", line 393, in <module>
[rank0]:     with torch.autocast(device_type=device, dtype=torch.bfloat16):
[rank0]:   File "/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 241, in __init__
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError: User specified an unsupported autocast device_type 'cuda:0'
è§£å†³ï¼šwith torch.autocast(device_type=device, dtype=torch.float16):
```

å› æ­¤ï¼Œå–æ¶ˆä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒã€‚

PyTorch è‡ªåŠ¨æ··åˆç²¾åº¦æ–‡æ¡£ï¼šhttps://pytorch.org/docs/stable/notes/amp_examples.html#autocast-and-custom-autograd-functions

### OMP_NUM_THREADS

```shell
$ Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
```

```shell
$ OMP_NUM_THREADS=8 nohup torchrun --standalone --nproc_per_node=2 train_gpt2.py > train_gpt2_64_1024_node2.log 2>&1 &
```

## æ€è€ƒ

### æŸå¤±å‡½æ•°

å°† Next Token Prediction è§†ä¸ºä¸€ä¸ª **å¤šåˆ†ç±»ï¼ˆvocab_sizeï¼‰é—®é¢˜**ï¼Œæ¨¡å‹è¾“å‡ºçš„ logits å°±æ˜¯ vocabulary ä¸­æ¯ä¸ªè¯å¯èƒ½çš„æ¦‚ç‡ï¼Œè€Œ target å°±æ˜¯æ­£ç¡®åˆ†ç±»å¯¹åº”çš„ vocabulary ä¸‹æ ‡ã€‚

ä½¿ç”¨ Cross Entropy æŸå¤±å‡½æ•°ï¼š

$$loss = -\sum_i\text{logits}_i\log \text{target}$$

å½“æ¨¡å‹çš„ç»“æœè¶Šæ¥è¶Šå¥½æ—¶ï¼Œlogits è¶Šä¼šæ¥è¿‘äº **ont-hot**ã€‚

```python
targets = x[1:] # (B, T)

logits = self.lm_head(x) # (B, T, vocab_size)
loss = None
if targets is not None:
    # logits: (B*T, vocab_size)
    # target: (B*T) -> (B*T, 1)
    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
```

### Pre-LN

GPT-2 ä½¿ç”¨ Pre-LN çš„å½¢å¼ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š

![Post-LN VS Pre-LN](/images/wiki/post-norm-and-pre-norm-comparison.png)

## å‚è€ƒ

- æœºå™¨ä¹‹å¿ƒï¼š[Karpathy æœ€æ–°å››å°æ—¶è§†é¢‘æ•™ç¨‹ï¼šä»é›¶å¤ç° GPT-2ï¼Œé€šå®µè¿è¡Œå³æå®š](https://www.jiqizhixin.com/articles/2024-06-11-8)

- CSDNï¼š[æ‰‹æ“ GPT ç³»åˆ—ä¹‹ - Logistic Regression æ¨¡å‹ï¼ŒSoftmax æ¨¡å‹çš„æŸå¤±å‡½æ•°ä¸ CrossEntropyLoss çš„å…³ç³»](https://blog.csdn.net/marlinlm/article/details/130167585)


### è§†é¢‘

We reproduce the GPT-2 (124M) from scratch. This video covers the whole process: First we build the GPT-2 network, then we optimize its training to be really fast, then we set up the training run following the GPT-2 and GPT-3 paper and their hyperparameters, then we hit run, and come back the next morning to see our results, and enjoy some amusing model generations. Keep in mind that in some places this video builds on the knowledge from earlier videos in the Zero to Hero Playlist (see my channel). You could also see this video as building my nanoGPT repo, which by the end is about 90% similar.

Links:
- build-nanogpt GitHub repo, with all the changes in this video as individual commits: https://github.com/karpathy/build-nan...

- nanoGPT repo: https://github.com/karpathy/nanoGPT

- llm.c repo: https://github.com/karpathy/llm.c

- my website: https://karpathy.ai

- my twitter:   / karpathy  

- our Discord channel:   / discord  

Supplementary links:

- Attention is All You Need paper: https://arxiv.org/abs/1706.03762

- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165 

- OpenAI GPT-2 paper: https://d4mucfpksywv.cloudfront.net/b... The GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand GPU instance in the cloud that you can ssh to: https://lambdalabs.com 

Chapters:

00:00:00 intro: Letâ€™s reproduce GPT-2 (124M)

00:03:39 exploring the GPT-2 (124M) OpenAI checkpoint

00:13:47 SECTION 1: implementing the GPT-2 nn.Module

00:28:08 loading the huggingface/GPT-2 parameters

00:31:00 implementing the forward pass to get logits

00:33:31 sampling init, prefix tokens, tokenization

00:37:02 sampling loop

00:41:47 sample, auto-detect the device

00:45:50 letâ€™s train: data batches (B,T) â†’ logits (B,T,C)

00:52:53 cross entropy loss

00:56:42 optimization loop: overfit a single batch

01:02:00 data loader lite

01:06:14 parameter sharing wte and lm_head

01:13:47 model initialization: std 0.02, residual init

01:22:18 SECTION 2: Letâ€™s make it fast. GPUs, mixed precision, 1000ms

01:28:14 Tensor Cores, timing the code, TF32 precision, 333ms

01:39:38 float16, gradient scalers, bfloat16, 300ms

01:48:15 torch.compile, Python overhead, kernel fusion, 130ms

02:00:18 flash attention, 96ms

02:06:54 nice/ugly numbers. vocab size 50257 â†’ 50304, 93ms

02:14:55 SECTION 3: hyperpamaters, AdamW, gradient clipping

02:21:06 learning rate scheduler: warmup + cosine decay

02:26:21 batch size schedule, weight decay, FusedAdamW, 90ms

02:34:09 gradient accumulation

02:46:52 distributed data parallel (DDP)

03:10:21 datasets used in GPT-2, GPT-3, FineWeb (EDU)

03:23:10 validation data split, validation loss, sampling revive

03:28:23 evaluation: HellaSwag, starting the run

03:43:05 SECTION 4: results in the morning! GPT-2, GPT-3 repro

03:56:21 shoutout to llm.c, equivalent but faster code in raw C/CUDA

03:59:39 summary, phew, build-nanogpt github repo

Corrections:
I will post all errata and followups to the build-nanogpt GitHub repo (link above)

SuperThanks:
I experimentally enabled them on my channel yesterday. Totally optional and only use if rich. All revenue goes to to supporting my work in AI + Education.



