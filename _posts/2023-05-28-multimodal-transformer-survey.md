---
layout: post
author: bookstall
tags: Multimodal
categories: [Multimodal]
excerpt: Multimodal Transformer 综述
keywords: Multimodal
title: Multimodal Transformer 综述
mathjax: true
sticky: false
---

> 《Multimodal Learning with Transformers: A Survey》
>
> - URL：https://arxiv.org/abs/2206.06488
>
> - 单位：Tsinghua University & University of Surrey & University of Oxford
>
> - 会议：TPAMI 2022
>
> 



<a href="https://pic2.zhimg.com/80/v2-bb8e5f1826eefee2306f6b9a204987c1_720w.webp" data-fancybox="images" data-caption="Transformer 的标准结构"><img src="https://pic2.zhimg.com/80/v2-bb8e5f1826eefee2306f6b9a204987c1_720w.webp" alt="Transformer 的标准结构" style="
    zoom: 50%;
"></a>



多模态 Transformer 的各种预训练代理任务如下图所示：

![](https://pic4.zhimg.com/80/v2-f375202decb8486e42bf714ea477c2af_720w.webp)

多模态 Transformer 对不同输入模态采取的 Tokenization 和 Token Embedding 方式如下图所示：

![](/images/posts/Multimodal-Transformer-Survey/tokenization-and-token-embedding-in-multimodal.png)




## 模态融合方式

![六种模态融合方式的示意图](https://pic4.zhimg.com/80/v2-4f55a9338e459570e6d3079d26219c07_720w.webp)


![六种模态融合方式的计算过程](/images/posts/Multimodal-Transformer-Survey/self-attention-variants-in-multimodal.png)





## 参考

- 论文：[Multimodal Learning with Transformers: A Survey](https://arxiv.org/abs/2206.06488)

