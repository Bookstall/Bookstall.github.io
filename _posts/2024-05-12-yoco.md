---
layout: post
author: bookstall
tags: Transformer
categories: [LLM]
excerpt: YOCO：You Only Cache Once
keywords: LLM
title: YOCO：You Only Cache Once
mathjax: true
sticky: False
---

## 简介

![](https://img.ithome.com/newsuploadfiles/2024/5/480324dc-88ca-429a-9bcd-068109e084c6.png?x-bce-process=image/format,f_auto)

微软 & 清华最新研究，打破 GPT 系列开创的 Decoder-Only 架构 —— 提出 Decoder-Decoder 新型架构，名为 YOCO（You Only Cache Once）。

YOCO 仅缓存一次键值对，可大幅降低 GPU 内存需求，且保留全局注意力能力。一张图来看 YOCO 和标准 Transformer 的比较。

![](https://img.ithome.com/newsuploadfiles/2024/5/d311a750-b97b-4ac0-8573-6ac96c72c12b.png?x-bce-process=image/format,f_auto)

在处理 512K 上下文长度时，标准 Transformer 内存使用是 YOCO 的 6.4 倍，预填充延迟是 YOCO 的 30.3 倍，而 YOCO 的吞吐量提升到标准 Transformer 的 9.6 倍。


TODO

## YOCO 架构

YOCO 整体架构设计如下所示，分为自解码器（Self-Decoder）和交叉解码器（Cross-Decoder）两部分。

![YOCO 架构图](https://img.ithome.com/newsuploadfiles/2024/5/0c27e541-48b7-4282-9b01-24c3b934a99c.png?x-bce-process=image/format,f_auto)

具体来说，YOCO 由 $$L$$ 个块堆叠而成，其中前 $$L / 2$$ 层是自解码器，其余模块是交叉解码器。

- 自解码器利用 **高效自注意力（efficient self-attention）** 机制来获取 KV 缓存：

  - 接收输入序列的嵌入表示，并使用高效自注意力来生成中间向量表示；
  
  - 使用因果掩码（causal masking）保证解码的自回归特性；
  
  - 自解码器的输出用于生成全局 KV 缓存；

- 而交叉解码器使用 **交叉注意力（cross-attention）**来重用自解码器生成的共享 KV 缓存：

  - 在自解码器生成的 KV 缓存基础上进行堆叠，以获得最终的输出向量；
  
  - 同样使用因果掩码来维持自回归生成；
  
  - 允许交叉解码器层间高效地重用 KV 缓存，减少了对 GPU 内存的需求；

总的来说，自解码器和交叉解码器的模块设计与 Transformer 的解码器层类似，包含交错注意力和前馈网络子层。不过，研究人员还进行了 **RMSNorm、SwiGLU 和分组查询注意力** 等改进。

---

两部分之间的区别在于注意力模块。

- 自解码器使用高效自注意力，如 **滑动窗口注意力（Sliding-Window Attention）** 或 **门控保留（gated retention）**。

- 而交叉解码器使用 **标准的多头交叉注意力**，Query 向量通过注意力与自解码器产生的全局键值缓存相关联。

## Self-Decoder：高效的注意力

TODO

### Gate Retention（门控保留）

TODO



### Sliding-Window Attention（滑动窗口注意力）

与 Matrix-8x7B 所使用的注意力一样。

TODO



## Cross-Decoder：标准的注意力

### Cross 的来源

Cross 的来源：

- Query 是来源于输入

- 而 Key、Value 不是来源于原来的输入，因此也正是这里称为 "Cross Attention" 的原因


666，周一去细读一下论文😅，上周刚感叹 DeepSeek-V2 的MLA的巧妙，结果周末又来了一个大的，不愧是微软


## 推理优势

YOCO 模型 **只缓存一层全局的键值对**，因此与 Transformer 模型相比，它需要的内存约少了 $$L$$（指模型的层数）倍。

![KV Cache](https://img.ithome.com/newsuploadfiles/2024/5/e07d6c16-6777-4842-884f-2fb8af2312fc.png?x-bce-process=image/format,f_auto)

## 参考

- DeepSeek-V2：

  - 论文：[You Only Cache Once: Decoder-Decoder Architectures for Language Models](https://arxiv.org/abs/2405.05254)

  - Github：[YOCO](https://github.com/microsoft/unilm/tree/master/YOCO)

- IT 之家：[微软打破 Decoder-Only 架构：大幅降低 GPU 内存需求](https://www.ithome.com/0/767/340.htm)





惠科股份

中谷海运补录
软件开发
华南片区：HNHR@zhonggu56.com
意向工作地+意向岗位+姓名+学校+生源地

启明信息补录：
http://zhaopin.faw.com.cn/

武汉新创元半导体








