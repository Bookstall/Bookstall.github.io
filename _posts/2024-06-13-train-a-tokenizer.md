---
layout: post
author: bookstall
tags: LLM
categories: [LLM]
excerpt: å½“è®­ç»ƒ or å¾®è°ƒä¸€ä¸ª LLM æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦è‡ªå·±è®­ç»ƒä¸€ä¸ª Tokenizerã€‚
keywords: LLM
title: è®­ç»ƒä¸€ä¸ª Tokenizer
mathjax: true
---


## 1ã€èƒŒæ™¯



### 1.1ã€Tokenizer çš„ä½œç”¨


### 1.2ã€Tokenizer çš„ç§ç±»



## 2ã€HuggingFace Tokenizer çš„å¤„ç†æµç¨‹

Tokenizer åŒ…æ‹¬å‡ ä¸ªæ­¥éª¤ï¼š

- è§„èŒƒåŒ–ï¼ˆä»»ä½•è®¤ä¸ºå¿…è¦çš„æ–‡æœ¬æ¸…ç†ï¼Œä¾‹å¦‚åˆ é™¤ç©ºæ ¼æˆ–é‡éŸ³ç¬¦å·ã€Unicode è§„èŒƒåŒ–ç­‰ï¼‰

- é¢„æ ‡è®°åŒ–ï¼ˆå°†è¾“å…¥æ‹†åˆ†ä¸ºå•è¯ï¼‰

- é€šè¿‡æ¨¡å‹å¤„ç†è¾“å…¥ï¼ˆä½¿ç”¨é¢„å…ˆæ‹†åˆ†çš„è¯æ¥ç”Ÿæˆä¸€ç³»åˆ—æ ‡è®°ï¼‰

- åå¤„ç†ï¼ˆæ·»åŠ æ ‡è®°å™¨çš„ç‰¹æ®Šæ ‡è®°ï¼Œç”Ÿæˆæ³¨æ„åŠ›æ©ç å’Œæ ‡è®°ç±»å‹ IDï¼‰

Tokenizer çš„å¤„ç†æµç¨‹å¦‚å›¾æ‰€ç¤ºï¼š

![Tokenizerçš„å¤„ç†æµç¨‹ç¤ºæ„å›¾](/images/posts/Tokenizer/tokenizer_processing.png)

> å›¾ç‰‡æ¥æºï¼šhttps://jinhanlei.github.io/posts/Transformers%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8-%E4%BA%8C-%E7%94%A8Tokenizer%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83%E8%AF%8D%E8%A1%A8/

![HuggingFace Tokenizerçš„å¤„ç†æµç¨‹ç¤ºæ„å›¾](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg)

> å›¾ç‰‡æ¥æºï¼šhttps://huggingface.co/learn/nlp-course/zh-CN/chapter6/4

æˆ‘ä»¬é€šå¸¸æŠŠè®­ç»ƒé›†å«åš â€œè¯­æ–™åº“â€ï¼ˆCorpusï¼‰ï¼Œæ ¹æ®è¯­æ–™åº“å¾—åˆ°ä¸€ä¸ªè¯è¡¨ï¼Œè¯è¡¨éœ€è¦æ¶µç›–è¯­æ–™åº“ä¸­çš„æ‰€æœ‰è¯ã€‚ä¸‹é¢è¯¦è§£è¿™ä¸€è¯è¡¨çš„æ„å»ºæµç¨‹ã€‚

### 2.1ã€Normalizersï¼ˆè§„èŒƒåŒ–ï¼‰

### 2.2ã€Pre-tokenizersï¼ˆé¢„åˆ†è¯ï¼‰

è¿™ä¸€æ­¥è¿›è¡Œé¢„åˆ†è¯ã€‚æ ¹æ®åˆ‡åˆ†ç²’åº¦çš„ä¸åŒï¼Œå¤©ç„¶å¯ä»¥æŠŠè‹±æ–‡å•è¯æ‹†æˆï¼šå­—ç¬¦ã€è¯ï¼Œä½†è¿™æ ·æœ‰å¾ˆå¤§çš„å¼Šç«¯ã€‚

#### 2.2.1ã€æŒ‰å­—ç¬¦åˆ‡åˆ†ï¼ˆCharacter-basedï¼‰

æŠŠæ–‡æœ¬åˆ‡åˆ†ä¸ºå­—ç¬¦ï¼Œè¿™æ ·å°±åªä¼šäº§ç”Ÿä¸€ä¸ªéå¸¸å°çš„è¯è¡¨ï¼Œæ¯”å¦‚è‹±æ–‡å°±åªæœ‰ 26 ä¸ªå­—æ¯å’Œæ ‡ç‚¹ç­‰ï¼Œå¾ˆå°‘ä¼šå‡ºç°è¯è¡¨å¤–çš„ tokensã€‚ä¾‹å¦‚å¯¹ `Attention is all we need!` æŒ‰å­—ç¬¦åˆ‡åˆ†ä¸ºï¼š

```shell
A|t|t|e|n|t|i|o|n|i|s|a|l|l|w|e|n|e|e|d|!
```

ä½†è¿™æ ·æ˜¾ç„¶ä¸å¤ªåˆç†ï¼Œå› ä¸ºå­—ç¬¦æœ¬èº«å¹¶æ²¡æœ‰å¤ªå¤šä¸Šä¸‹æ–‡çš„è§„å¾‹ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™æ ·ä»…ä»…ä¸€ä¸ªå•è¯å°±éœ€è¦ä¸€å †å‘é‡å»è¡¨ç¤ºï¼Œé‡åˆ°é•¿æ–‡æœ¬æ—¶å°±çˆ†ç‚¸äº†ã€‚

> åˆ’åˆ†çš„ token å¤ªé•¿ï¼Œè®¡ç®— attention æ—¶æ¶ˆè€—å¤ªå¤šæ—¶é—´

#### 2.2.2ã€æŒ‰è¯åˆ‡åˆ†ï¼ˆWord-basedï¼‰

æŒ‰è¯åˆ‡åˆ†å‡ ä¹æ˜¯æœ€ç›´è§‚çš„æ–¹æ³•ã€‚

```shell
Attention|is|all|we|need|!
```

è¿™ç§ç­–ç•¥ä¹ŸåŒæ ·å­˜åœ¨é—®é¢˜ã€‚

- å¯¹äºä¸­æ–‡ï¼Œå› ä¸ºå­—ä¹‹é—´æ²¡æœ‰ç©ºæ ¼å¤©ç„¶åˆ†å¼€æˆè¯ï¼Œåˆ†è¯æœ¬èº«å°±æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚

- å¯¹äºè‹±æ–‡ï¼Œä¼šå°†æ–‡æœ¬ä¸­æ‰€æœ‰å‡ºç°è¿‡çš„ç‹¬ç«‹ç‰‡æ®µéƒ½ä½œä¸ºä¸åŒçš„ tokenï¼Œä»è€Œäº§ç”Ÿå·¨å¤§çš„è¯è¡¨ï¼Œè€Œå®é™…ä¸Šè¯è¡¨ä¸­å¾ˆå¤šè¯æ˜¯ç›¸å…³çš„ï¼Œä¾‹å¦‚ â€œdogâ€ å’Œ â€œdogsâ€ã€â€œrunâ€ å’Œ â€œrunningâ€ï¼Œå¦‚æœè§†ä½œä¸åŒçš„è¯ï¼Œå°±æ— æ³•è¡¨ç¤ºå‡ºè¿™ç§å…³è”æ€§ã€‚

å¹¶ä¸”ï¼Œè¿™æ ·ä¼šå‡ºç° **OOVï¼ˆOut of vocabularyï¼‰**é—®é¢˜ï¼Œåœ¨é¢„æµ‹æ—¶æœ‰å¯èƒ½é‡åˆ°è¯­æ–™åº“ä¸­ä»æ²¡å‡ºç°è¿‡çš„è¯ï¼Œåˆ†è¯å™¨ä¼šä½¿ç”¨ä¸€ä¸ªä¸“é—¨çš„ `[UNK]` token æ¥è¡¨ç¤ºï¼Œå¦‚æœè®­ç»ƒé›†ä¸å¤Ÿå¤§ï¼Œè¯è¡¨é‡Œå°±é‚£ä¹ˆå‡ ä¸ªè¯ï¼Œé‚£ä¹ˆç”¨çš„æ—¶å€™ï¼Œå¥å­ä¸­ä¼šåŒ…å«å¤§é‡ `[UNK]`ï¼Œå¯¼è‡´å¤§é‡ä¿¡æ¯ä¸§å¤±ã€‚å› æ­¤ï¼Œä¸€ä¸ªå¥½çš„åˆ†è¯ç­–ç•¥ï¼Œåº”è¯¥å°½å¯èƒ½ä¸å‡ºç° `[UNK]`ã€‚



#### 2.2.3ã€æŒ‰å­è¯åˆ‡åˆ†ï¼ˆSubword-basedï¼‰

ç°åœ¨ï¼Œå¹¿æ³›é‡‡ç”¨çš„æ˜¯ä¸€ç§åŒæ—¶ç»“åˆäº†æŒ‰è¯åˆ‡åˆ†å’ŒæŒ‰å­—ç¬¦åˆ‡åˆ†çš„æ–¹å¼â€”â€”æŒ‰å­è¯åˆ‡åˆ† (Subword tokenization)ã€‚

BERTã€GPT éƒ½é‡‡ç”¨è¿™ç§åšæ³•ï¼Œ**é«˜é¢‘è¯ç›´æ¥ä¿ç•™ï¼Œä½é¢‘è¯è¢«åˆ‡åˆ†ä¸ºå­è¯**ã€‚

Subword ç®—æ˜¯ä¸€ç§å¯¹å­—ç¬¦å’Œè¯æŠ˜ä¸­çš„åŠæ³•ã€‚ä¸ä»…å­è¯ä¹‹é—´æœ‰è§„å¾‹å¯å¾ªã€å•è¯ä¸ä¼šåˆ‡çš„è¿‡åˆ†é•¿ï¼Œè€Œä¸”åªç”¨ä¸€ä¸ªè¾ƒå°çš„è¯è¡¨å°±å¯ä»¥è¦†ç›–ç»å¤§éƒ¨åˆ†çš„æ–‡æœ¬ï¼ŒåŸºæœ¬ä¸ä¼šäº§ç”Ÿ `[UNK]`ã€‚

åœ¨ç»Ÿè®¡è¯é¢‘å‰ï¼Œéœ€è¦é¢„å…ˆåˆ‡å‰²è¯ï¼Œåˆ‡å®Œæ‰èƒ½å»ç»Ÿè®¡ï¼ŒPre-tokenizers å°±æ˜¯åˆ‡å‰²çš„ä½œç”¨ã€‚è‹±æ–‡æœ¬èº«å°±å¯ä»¥æ ¹æ®ç©ºæ ¼åˆ†å‰²ï¼Œä½†æ˜¯å¯¹äºä¸­æ—¥éŸ©è¿™æ ·çš„è¿ç»­å­—ç¬¦ï¼Œå¦‚æœæŒ‰å•å­—åˆ‡å®Œï¼Œè¯è¡¨å°†ä¼šå·¨å¤§ï¼Œå‡ ä¹éœ€è¦ 130,000+ ä¸ª Unicode å­—ç¬¦ï¼Œæ›´åˆ«è¯´è¿˜è¦ç»§ç»­ç»„è¯äº†ï¼

---

äºæ˜¯ï¼ŒGPT-2 ç­‰é‡‡ç”¨äº† ByteLevel çš„ç®—æ³•ï¼Œå°†ä¸­æ—¥éŸ©ç­‰å­—ç¬¦æ˜ å°„åˆ° **256 ä¸ªå­—ç¬¦**ã€‚å…·ä½“å¯ä»¥å‚è€ƒ [fairseq](https://github.com/facebookresearch/fairseq/blob/main/fairseq/data/encoders/byte_utils.py) å’Œ [icefall](https://github.com/k2-fsa/icefall/blob/master/icefall/byte_utils.py)ã€‚å°† [fairseq](https://github.com/facebookresearch/fairseq/blob/main/fairseq/data/encoders/byte_utils.py) é‡Œé¢çš„ä»£ç  copy ä¸‹æ¥ï¼ŒåŠ ä¸Šï¼š

```python
enc = byte_encode("å”±ã€è·³ã€rapã€ç¯®çƒ")
print(enc)
print(byte_decode(enc))
print(len(BYTE_TO_BCHAR))
```

`å”±ã€è·³ã€rapã€ç¯®çƒ` è¢«ç¼–ç ä¸º `Ã¥Æ”Â±Ã£Æ€ÆÃ¨Â·Â³Ã£Æ€ÆrapÃ£Æ€ÆÃ§Â¯Â®Ã§ÆÆƒ`ï¼Œè¿™å•¥ï¼Ÿæˆ‘ä¹Ÿçœ‹ä¸æ‡‚ï¼Œä½†æ˜¯æ‰¾åˆ° `rap` åœ¨é‡Œé¢äº†å—ï¼Ÿè‹±æ–‡æ˜¯ä¸å˜çš„ï¼Œæ¯ä¸ªä¸­æ–‡å­—ç¬¦ä¼šè¢«æ˜ å°„æˆä¸‰ä¸ª `Ã¥` è¿™ç§ç©æ„å„¿ï¼Œè€Œè¿™ç§ç©æ„å„¿åŠ ä¸Šç©ºæ ¼ã€è‹±æ–‡å­—æ¯å’Œæ ‡ç‚¹ç­‰ï¼Œä¸€å…±åªæœ‰ 256 ä¸ªï¼Œä»–ä»¬çš„æ’åˆ—ç»„åˆå¯ä»¥æ„æˆå‡ ä¹æ‰€æœ‰å­—ç¬¦ã€‚

ä¹‹åå»ç»Ÿè®¡è¿™äº›ç©æ„å„¿çš„å…±ç°é¢‘ç‡å°±å¯ï¼Œä½†è€ƒè™‘åˆ°å¯èƒ½ä¼šæœ‰ä¸€äº›æ— æ•ˆçš„ç»„åˆï¼Œæ¯”å¦‚ä¸­é—´å°‘ä¸ª `Æ”`ï¼Œ`byte_decode` å°±å‡ºä¸æ¥äº†ï¼Œäºæ˜¯å¼•å…¥åŸºäºåŠ¨æ€è§„åˆ’çš„æœ€ä½³è§£ç æ–¹æ³• `smart_byte_decode`ï¼š

```python
wrong_byte = "Ã¥Â±Ã£Æ€ÆÃ¨Â·Â³Ã£Æ€ÆrapÃ£Æ€ÆÃ§Â¯Â®Ã§ÆÆƒ"
print(byte_decode(wrong_byte))
print(smart_byte_decode(wrong_byte))
```

å°†æ¯ä¸ªæ±‰å­—ç”¨ [ç©ºæ ¼åˆ†éš”](https://github.com/k2-fsa/icefall/blob/dca21c2a17b6e62f687e49398517cb57f62203b0/icefall/utils.py#L1370)ï¼Œå°±å¯ä»¥æŒ‰è‹±æ–‡é‚£èˆ¬åˆ†è¯ã€‚åšå®Œåˆ†è¯ï¼Œå°±å¯ä»¥æ¥ç»Ÿè®¡è¯é¢‘äº†ï¼Œè®­ç»ƒä¸€ä¸ªè¯è¡¨å‡ºæ¥äº†ã€‚

### 2.3ã€Tokenizer-Modelsï¼ˆæ„å»ºè¯è¡¨çš„ç»Ÿè®¡æ¨¡å‹ï¼‰

Tokenizer çš„ Models æ˜¯æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼ŒæŒ‡æ„å»ºè¯è¡¨çš„ç»Ÿè®¡æ¨¡å‹ï¼Œæœ‰ä¸‰å¤§å­è¯æ ‡è®°åŒ–ç®—æ³•ï¼šBPEã€WordPiece å’Œ Unigramã€‚

åœ¨ä¸‹é¢çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥ç ”ç©¶ä¸‰ç§ä¸»è¦çš„å­è¯æ ‡è®°åŒ–ç®—æ³•ï¼š**BPE**ï¼ˆç”± GPT-2 å’Œå…¶ä»–äººä½¿ç”¨ï¼‰ã€**WordPiece**ï¼ˆä¾‹å¦‚ç”± BERT ä½¿ç”¨ï¼‰å’Œ **Unigram**ï¼ˆç”± T5 å’Œå…¶ä»–äººä½¿ç”¨ï¼‰ã€‚åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè¿™é‡Œæ˜¯å®ƒä»¬å„è‡ªå·¥ä½œåŸç†çš„å¿«é€Ÿæ¦‚è¿°ã€‚å¦‚æœæ‚¨è¿˜æ²¡æœ‰ç†è§£ï¼Œè¯·åœ¨é˜…è¯»ä¸‹ä¸€èŠ‚åç«‹å³å›åˆ°æ­¤è¡¨ã€‚


| Model         | BPE                                                            | WordPiece                                                                                               | Unigram                                                                       |
| ------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| Training      | ä»å°‘é‡è¯æ±‡å¼€å§‹ï¼Œå­¦ä¹ åˆå¹¶ token çš„è§„åˆ™                          | ä»å°‘é‡è¯æ±‡å¼€å§‹ï¼Œå­¦ä¹ åˆå¹¶ token çš„è§„åˆ™                                                                   | ä»å¤§é‡è¯æ±‡å¼€å§‹ï¼Œå­¦ä¹ åˆ é™¤ token çš„è§„åˆ™                                         |
| Training step | åˆå¹¶æœ€å¸¸è§çš„ç›¸é‚» token å¯¹                                      | æ ¹æ®å¯¹çš„é¢‘ç‡åˆå¹¶ä¸å…·æœ‰æœ€ä½³åˆ†æ•°çš„å¯¹ç›¸å¯¹åº”çš„ä»¤ç‰Œï¼Œä½¿æ¯ä¸ªå•ç‹¬ token çš„é¢‘ç‡è¾ƒä½ï¼ˆé¢‘ç‡ä½çš„åšä¸ºå•ç‹¬çš„ tokenï¼‰ | åˆ é™¤è¯æ±‡è¡¨ä¸­çš„æ‰€æœ‰ tokenï¼Œè¿™äº› token å°†æœ€å¤§ç¨‹åº¦åœ°å‡å°‘åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸Šè®¡ç®—çš„æŸå¤± |
| Learns        | åˆå¹¶è§„åˆ™å’Œè¯æ±‡è¡¨                                               | åªæ˜¯ä¸€ä¸ªè¯æ±‡                                                                                            | åŒ…å«æ¯ä¸ª token åˆ†æ•°çš„è¯æ±‡è¡¨                                                   |
| Encoding      | å°†å•è¯æ‹†åˆ†ä¸ºå¤šä¸ªå­—ç¬¦ï¼ˆcharactersï¼‰ï¼Œå¹¶ä½¿ç”¨åœ¨è®­ç»ƒæœŸé—´å­¦åˆ°çš„åˆå¹¶ | æŸ¥æ‰¾ä»è¯æ±‡è¡¨ä¸­çš„å¼€å¤´å¼€å§‹çš„æœ€é•¿çš„å­å­—ï¼ˆsubwordï¼‰ï¼Œç„¶åå¯¹å•è¯çš„å…¶ä½™éƒ¨åˆ†æ‰§è¡Œç›¸åŒçš„æ“ä½œ                     | ä½¿ç”¨è®­ç»ƒæœŸé—´å­¦åˆ°çš„åˆ†æ•°æ‰¾åˆ°æœ€æœ‰å¯èƒ½è¢«æ‹†åˆ†çš„ token                              |

#### 2.3.1ã€Byte Pair Encodingï¼ˆBPEï¼‰

> å‚è€ƒï¼š
>
> - HuggingFaceï¼šhttps://huggingface.co/learn/nlp-course/zh-CN/chapter6/5?fw=pt

å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰æœ€åˆè¢«å¼€å‘ä¸ºä¸€ç§å‹ç¼©æ–‡æœ¬çš„ç®—æ³•,ç„¶ååœ¨é¢„è®­ç»ƒ GPT æ¨¡å‹æ—¶è¢« OpenAI ç”¨äºæ ‡è®°åŒ–ã€‚è®¸å¤š Transformer æ¨¡å‹éƒ½ä½¿ç”¨å®ƒ,åŒ…æ‹¬ GPTã€GPT-2ã€RoBERTaã€BART å’Œ DeBERTaã€‚

[BPE](https://arxiv.org/abs/1508.07909) ç®€å•æœ‰æ•ˆï¼Œæ˜¯ç›®å‰æœ€æµè¡Œçš„æ–¹æ³•ä¹‹ä¸€ï¼ŒGPT-2 å’Œ RoBERTa ä½¿ç”¨çš„ Subword ç®—æ³•éƒ½æ˜¯ BPEã€‚BPE çš„æµç¨‹å¦‚ä¸‹ï¼š

- æ ¹æ®åˆ†è¯ç»“æœç»Ÿè®¡è¯é¢‘ï¼Œå¾—åˆ°{`è¯ </w>`: è¯é¢‘}ï¼Œè¯ååŠ ä¸Šæœ«å°¾ç¬¦æ˜¯ä¸ºäº†åŒºåˆ† â€œ**est**imateâ€ å’Œ â€œhigh**est**â€ è¿™ç±»è¯ï¼›

- ç»Ÿè®¡å­—ç¬¦çš„ä¸ªæ•°ï¼ˆæ¯”å¦‚é‚£ 256 ä¸ªç©æ„ï¼‰ï¼Œå¾—åˆ° {â€œå­—ç¬¦â€: å­—ç¬¦é¢‘} è¡¨ï¼›

- æ‹¿å­—ç¬¦é¢‘æœ€é«˜çš„å­—ç¬¦ä¸ä¸‹ä¸€å­—ç¬¦åˆå¹¶ï¼Œç»Ÿè®¡åˆå¹¶åçš„ Subword é¢‘ç‡ï¼›

- å°† Subword é¢‘æ·»åŠ åˆ°{â€œå­—ç¬¦â€: å­—ç¬¦é¢‘}è¡¨ï¼Œè¿™æ—¶è¯è¡¨ä¼šæ‰©å¤§ï¼›

- ç»§ç»­æ‹¿è¡¨ä¸­é¢‘ç‡æœ€é«˜çš„å»åˆå¹¶ï¼Œåˆ°æœ«å°¾ç¬¦æ—¶åœæ­¢è¿™ä¸ªè¯çš„åˆå¹¶ï¼›

- é‡å¤ç›´åˆ°é¢„è®¾çš„è¯è¡¨å¤§å°æˆ–æœ€é«˜é¢‘æ•°ä¸º 1ã€‚

æ›´è¯¦å°½çš„æ¨å¯¼å¯ä»¥å‚è€ƒ [è¿™ç¯‡](https://zhuanlan.zhihu.com/p/424631681)ã€‚


#### 2.3.2ã€WordPiece

> å‚è€ƒï¼š
>
> - HuggingFaceï¼šhttps://huggingface.co/learn/nlp-course/zh-CN/chapter6/6?fw=pt

WordPiece æ˜¯ Google ä¸ºé¢„è®­ç»ƒ BERT è€Œå¼€å‘çš„æ ‡è®°åŒ–ç®—æ³•ã€‚æ­¤å,å®ƒåœ¨ä¸å°‘åŸºäº BERT çš„ Transformer æ¨¡å‹ä¸­å¾—åˆ°é‡ç”¨,ä¾‹å¦‚ DistilBERTã€MobileBERTã€Funnel Transformers å’Œ MPNETã€‚å®ƒåœ¨è®­ç»ƒæ–¹é¢ä¸ BPE éå¸¸ç›¸ä¼¼,ä½†å®é™…æ ‡è®°åŒ–çš„æ–¹å¼ä¸åŒã€‚

WordPiece ä¸»è¦åœ¨ BERT ç±»æ¨¡å‹ä¸­ä½¿ç”¨ã€‚ä¸ BPE é€‰æ‹© **é¢‘æ•°æœ€é«˜çš„ç›¸é‚»å­è¯åˆå¹¶** ä¸åŒçš„æ˜¯ï¼ŒWordPiece é€‰æ‹© **èƒ½å¤Ÿæå‡è¯­è¨€æ¨¡å‹æ¦‚ç‡æœ€å¤§çš„ç›¸é‚»å­è¯** åŠ å…¥è¯è¡¨ã€‚

#### 2.3.3ã€Unigram

> å‚è€ƒï¼š
>
> - HuggingFaceï¼šhttps://huggingface.co/learn/nlp-course/zh-CN/chapter6/7?fw=pt

åœ¨ SentencePiece ä¸­ç»å¸¸ä½¿ç”¨ Unigram ç®—æ³•,è¯¥ç®—æ³•æ˜¯ AlBERTã€T5ã€mBARTã€Big Bird å’Œ XLNet ç­‰æ¨¡å‹ä½¿ç”¨çš„æ ‡è®°åŒ–ç®—æ³•ã€‚


[Unigram](https://arxiv.org/pdf/1804.10959.pdf) çš„æ“ä½œæ˜¯å’Œå‰ä¸¤è€…åå‘çš„ã€‚ä¸åŒäºæ‹¼è¯ï¼ŒUnigram æ˜¯å‰²è¯ï¼Œé¦–å…ˆåˆå§‹ä¸€ä¸ªå¤§è¯è¡¨ï¼Œæ¥ç€é€šè¿‡æ¦‚ç‡æ¨¡å‹ä¸æ–­æ‹†å‡ºå­è¯ï¼Œç›´åˆ°é™å®šè¯æ±‡é‡ã€‚

å¯ä»¥ä» WordPiece çš„å…¬å¼å»ç†è§£ã€‚ç”±äºåˆšå¼€å§‹éƒ½æ˜¯é•¿è¯ï¼Œè¯è¡¨æ˜¯å·¨å¤§çš„ï¼Œé€šè¿‡æ‹†æ¦‚ç‡å°çš„è¯ï¼Œä¿ç•™æ¦‚ç‡å¤§çš„è¯ï¼Œä»è€Œç¼©å°è¯è¡¨ã€‚

æ ¹æ®è¿™äº›æ–¹æ³•ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„è¯­æ–™ï¼Œè®­ç»ƒä¸€ä¸ªå‚ç›´é¢†åŸŸçš„è¯è¡¨ï¼Œè¿™äº›æ–¹æ³•èƒ½å¤Ÿå¾ˆå¥½åœ°å°†é«˜é¢‘è¯ã€æœ¯è¯­ç­‰ç»Ÿè®¡å‡ºæ¥ã€‚

### 2.4ã€Post-Processorsï¼ˆåå¤„ç†ï¼‰

åœ¨è®­ç»ƒè¯è¡¨åï¼Œè¿˜å¯èƒ½éœ€è¦å¯¹å¥å­è¿›è¡Œåå¤„ç†ã€‚ä¾‹å¦‚ä¸€äº›æ¨¡å‹å½“æˆ‘ä»¬åˆ†å®Œè¯ï¼Œè¿˜æƒ³ç»™å¥å­åŠ å…¥ç‰¹æ®Šçš„æ ‡è®°ï¼Œä¾‹å¦‚ BERT ä¼šç»™å¥å­åŠ å…¥åˆ†ç±»å‘é‡å’Œåˆ†éš”ç¬¦ `[CLS] My horse is amazing [SEP]`ï¼Œè¿™æ—¶å°±éœ€è¦Post-Processorsã€‚

## 3ã€ç»§æ‰¿ä¸€ä¸ªå·²æœ‰çš„ Tokenizer

> å‚è€ƒï¼š
>
> - HuggingFaceï¼šhttps://huggingface.co/learn/nlp-course/zh-CN/chapter6/2?fw=pt

### 3.1ã€å‡†å¤‡è¯­æ–™åº“



### 3.2ã€è®­ç»ƒæ–°çš„ Tokenizer



### 3.3ã€ä¿å­˜ Tokenizer

## 4ã€ä»å¤´å¼€å§‹è®­ç»ƒ HuggingFace Tokenizer

> å‚è€ƒï¼šhttps://huggingface.co/learn/nlp-course/zh-CN/chapter6/8?fw=pt

### 4.0ã€æ„å»ºè¯­æ–™åº“

ä½¿ç”¨ [WikiText-2](https://huggingface.co/datasets/wikitext) æ•°æ®é›†

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")

def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

`get_training_corpus()` å‡½æ•°æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡è°ƒç”¨çš„æ—¶å€™å°†äº§ç”Ÿ 1,000 ä¸ªæ–‡æœ¬ï¼Œæˆ‘ä»¬å°†ç”¨å®ƒæ¥è®­ç»ƒæ ‡è®°å™¨ã€‚

Tokenizers ä¹Ÿå¯ä»¥ç›´æ¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•ç”Ÿæˆä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°ä½¿ç”¨çš„æ¥è‡ª WikiText-2 çš„æ‰€æœ‰æ–‡æœ¬/è¾“å…¥ï¼š

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

### 4.1ã€WordPiece Tokenizer


### 4.2ã€BPE Tokenizer

ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ª GPT-2 æ ‡è®°å™¨ã€‚ä¸ BERT æ ‡è®°å™¨ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ Tokenizer åˆå§‹åŒ–ä¸€ä¸ªBPE æ¨¡å‹ï¼š

```python
tokenizer = Tokenizer(models.BPE())
```

å’Œ BERT ä¸€æ ·ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªè¯æ±‡è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªè¯æ±‡è¡¨æ¥åˆå§‹åŒ–è¿™ä¸ªæ¨¡å‹ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’ vocab å’Œ mergesï¼‰ï¼Œä½†æ˜¯ç”±äºæˆ‘ä»¬å°†ä»å¤´å¼€å§‹è®­ç»ƒï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦è¿™æ ·å»åšã€‚ æˆ‘ä»¬ä¹Ÿä¸éœ€è¦æŒ‡å®š â€œunk_tokenâ€ï¼Œå› ä¸º GPT-2 ä½¿ç”¨çš„å­—èŠ‚çº§ BPEï¼Œä¸éœ€è¦ â€œunk_tokenâ€ã€‚

---

GPT-2 ä¸ä½¿ç”¨å½’ä¸€åŒ–å™¨ï¼Œå› æ­¤æˆ‘ä»¬è·³è¿‡è¯¥æ­¥éª¤å¹¶ç›´æ¥è¿›å…¥é¢„æ ‡è®°åŒ–ï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

æˆ‘ä»¬åœ¨æ­¤å¤„æ·»åŠ åˆ° ByteLevel çš„é€‰é¡¹ `add_prefix_space=False` æ˜¯ä¸åœ¨å¥å­å¼€å¤´æ·»åŠ ç©ºæ ¼ï¼ˆé»˜è®¤ä¸º Trueï¼‰ã€‚ æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹ä½¿ç”¨è¿™ä¸ªæ ‡è®°å™¨å¯¹ä¹‹å‰ç¤ºä¾‹æ–‡æœ¬çš„é¢„æ ‡è®°ï¼š

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```shell
[('Let', (0, 3)), ("'s", (3, 5)), ('Ä test', (5, 10)), ('Ä pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```

---

æ¥ä¸‹æ¥æ˜¯éœ€è¦è®­ç»ƒçš„æ¨¡å‹ã€‚å¯¹äº GPT-2ï¼Œå”¯ä¸€çš„ç‰¹æ®Šæ ‡è®°æ˜¯æ–‡æœ¬ç»“æŸæ ‡è®°ï¼š

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])

tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

ä¸ WordPieceTrainer ä»¥åŠ `vocab_size` å’Œ `special_tokens` ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®š `min_frequency` å¦‚æœæˆ‘ä»¬æ„¿æ„ï¼Œæˆ–è€…å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªè¯å°¾åç¼€ï¼ˆå¦‚ `</w>`)ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `end_of_word_suffix` è®¾ç½®å®ƒã€‚

è¿™ä¸ªæ ‡è®°å™¨ä¹Ÿå¯ä»¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè®­ç»ƒï¼š

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç¤ºä¾‹æ–‡æœ¬çš„æ ‡è®°åŒ–åçš„ç»“æœï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```shell
['L', 'et', "'", 's', 'Ä test', 'Ä this', 'Ä to', 'ken', 'izer', '.']
```

---

æˆ‘ä»¬å¯¹ GPT-2 æ ‡è®°å™¨æ·»åŠ å­—èŠ‚çº§åå¤„ç†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```
`trim_offsets = False` é€‰é¡¹æŒ‡ç¤ºæˆ‘ä»¬åº”è¯¥ä¿ç•™ä»¥ â€˜Ä â€™ å¼€å¤´çš„æ ‡è®°çš„åç§»é‡ï¼šè¿™æ ·åç§»é‡çš„å¼€å¤´å°†æŒ‡å‘å•è¯ä¹‹å‰çš„ç©ºæ ¼ï¼Œè€Œä¸æ˜¯ç¬¬ä¸€ä¸ªå•è¯çš„å­—ç¬¦ï¼ˆå› ä¸ºç©ºæ ¼åœ¨æŠ€æœ¯ä¸Šæ˜¯æ ‡è®°çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚ è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åˆšåˆšç¼–ç çš„æ–‡æœ¬çš„ç»“æœï¼Œå…¶ä¸­ 'Ä test' æ˜¯ç´¢å¼•ç¬¬ 4 å¤„çš„æ ‡è®°ï¼š

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```shell
' test'
```

---

æœ€åï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªå­—èŠ‚çº§è§£ç å™¨ï¼š

```python
tokenizer.decoder = decoders.ByteLevel()
```

æˆ‘ä»¬å¯ä»¥ä»”ç»†æ£€æŸ¥å®ƒæ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```python
tokenizer.decode(encoding.ids)
```

```shell
"Let's test this tokenizer."
```

---

ç°åœ¨æˆ‘ä»¬å®Œæˆäº†ï¼Œæˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·ä¿å­˜æ ‡è®°å™¨ï¼Œå¹¶å°†å®ƒåŒ…è£…åœ¨ä¸€ä¸ª PreTrainedTokenizerFast æˆ–è€… GPT2TokenizerFast å¦‚æœæˆ‘ä»¬æƒ³åœ¨ ğŸ¤— Transformersä¸­ä½¿ç”¨å®ƒï¼š

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

æˆ–è€…ï¼š

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

### 4.3ã€Unigram Tokenizer



## 5ã€ä»é›¶å¼€å§‹è®­ç»ƒ BPE Tokenizer



## 6ã€å…³äº Tokenizer çš„æ€è€ƒ




## æ›´å¤š

- tiktokenizerï¼šhttps://tiktokenizer.vercel.app

- tiktoken from OpenAI: https://github.com/openai/tiktoken

- sentencepiece from Google https://github.com/google/sentencepiece


## å‚è€ƒ

- bilibiliï¼š

  - 

- çŸ¥ä¹ï¼š
  
  - 

- HuggingFaceï¼š[NLP Course](https://huggingface.co/learn/nlp-course/zh-CN/chapter6/1?fw=pt)

- Youtubeï¼š[Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)

  - [Google colab for the video](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing)

  - GitHub repo for the video: [minBPE](https://github.com/karpathy/minbpe)



