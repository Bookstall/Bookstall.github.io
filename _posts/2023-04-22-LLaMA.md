---
layout: post
author: bookstall
tags: AI, LLM
categories: [AI, LLM]
excerpt: LLaMA：Open and Efficient Foundation Language Models，是 Meta AI ("被迫") 开源的 LLaMA，又称 "大羊驼"。
keywords: AI, LLM
title: LLaMA：Open and Efficient Foundation Language Models
mathjax: true
---

## LLaMA：大羊驼

> 《LLaMA: Open and Efficient Foundation Language Models》
> 
> - URL：https://arxiv.org/abs/2302.13971v1
>
> - Official Code：https://github.com/facebookresearch/llama
>
> - 
>
> - 单位：Meta AI





## 实践

### 模型下载（泄露版）

- 可以直接 wget 下载的网站：[IPFS](https://ipfs.io/ipfs/Qmb9y5GCkTG7ZzbBWMu2BXwMkzyCKcUjtEKPpgdZ7GEFKm/)

- 123 云盘

  - [LLaMA-7B](https://www.123pan.com/s/sKd9-bBJc.html)（14 GB）

  - [LLaMA-13B](https://www.123pan.com/s/sKd9-yJJc.html)（26 GB）

  - [LLaMA-30B](https://www.123pan.com/s/sKd9-CMJc.html)（65 GB）

  - [LLaMA-65B](https://www.123pan.com/s/sKd9-8JJc.html)（131 GB）

  - [LLMA-Smallint](https://www.123pan.com/s/sKd9-sIJc.html)（包括 LLaMA-7B、13B、30B 的 3 bit 和 4 bit 版本）

- 使用 `wget` 指令下载（与上面 IPFS 的文件一致，只是下载地址不太一样）

  ```sh
  mkdir LLaMA-checkpoint
  cd LLaMA-checkpoint

  wget https://agi.gpt4.org/llama/LLaMA/tokenizer.model -O ./tokenizer.model
  wget https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk -O ./tokenizer_checklist.chk

  wget https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth -O ./7B/consolidated.00.pth
  wget https://agi.gpt4.org/llama/LLaMA/7B/params.json -O ./7B/params.json
  wget https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk -O ./7B/checklist.chk
  
  wget https://agi.gpt4.org/llama/LLaMA/13B/consolidated.00.pth -O ./13B/consolidated.00.pth
  wget https://agi.gpt4.org/llama/LLaMA/13B/consolidated.01.pth -O ./13B/consolidated.01.pth
  wget https://agi.gpt4.org/llama/LLaMA/13B/params.json -O ./13B/params.json
  wget https://agi.gpt4.org/llama/LLaMA/13B/checklist.chk -O ./13B/checklist.chk

  wget https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth -O ./30B/consolidated.00.pth
  wget https://agi.gpt4.org/llama/LLaMA/30B/consolidated.01.pth -O ./30B/consolidated.01.pth
  wget https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth -O ./30B/consolidated.02.pth
  wget https://agi.gpt4.org/llama/LLaMA/30B/consolidated.03.pth -O ./30B/consolidated.03.pth
  wget https://agi.gpt4.org/llama/LLaMA/30B/params.json -O ./30B/params.json
  wget https://agi.gpt4.org/llama/LLaMA/30B/checklist.chk -O ./30B/checklist.chk

  wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth -O ./65B/consolidated.00.pth
  wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.01.pth -O ./65B/consolidated.01.pth
  wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth -O ./65B/consolidated.02.pth
  wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.03.pth -O ./65B/consolidated.03.pth
  wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth -O ./65B/consolidated.04.pth
  wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.05.pth -O ./65B/consolidated.05.pth
  wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth -O ./65B/consolidated.06.pth
  wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth -O ./65B/consolidated.07.pth
  wget https://agi.gpt4.org/llama/LLaMA/65B/params.json -O ./65B/params.json
  wget https://agi.gpt4.org/llama/LLaMA/65B/checklist.chk -O ./65B/checklist.chk

  cd ..
  ```

---

除了 LLaMA-7B 模型，其他的模型参数文件均被分为多个分片，需要进行合并。

Windows 系统下：

```shell

```

Linux 系统下：

```shell

```

### 实战





## 其他二创项目

### pyllama：精简版 LLaMA

github repo：[pyllama](https://github.com/juncongmoo/pyllama)

- `pyllama` is a hacked version of LLaMA based on original Facebook's implementation but more convenient to run in a Single consumer grade GPU.

- Run LLM in A Single 4GB GPU


### llama.cpp：移植 LLaMA 到 C/C++

[llama.cpp](https://github.com/ggerganov/llama.cpp) 可以将 LLaMA 移植到 C/C++ 中，方便部署。

- Port of Facebook's LLaMA model in C/C++

- Inference of LLaMA model in pure C/C++

![llama.cpp logo](https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png)



## LLaMA "开源" 之后

谁能想到，一次意外的 LLaMA 泄漏，竟点燃了开源 LLM 领域最大的创新火花。

一系列表现出色的 ChatGPT 开源替代品——「羊驼家族」，随后眼花缭乱地登场。







## LLaMA 论文阅读

### 导言

> 使用 万亿 Tokens 数据（仅使用公开的、开源的数据）来训练 LLaMA 模型，包括 70 亿到 650 亿的参数量，

- We introduce **LLaMA**, a collection of foundation language models ranging <u>from 7B to 65B parameters</u>. 

- We **train our models on trillions of tokens**, and show that it is possible to train state-of-the-art models using **publicly available datasets** exclusively, without resorting to proprietary and inaccessible datasets. 

- In particular, **LLaMA-13B** outperforms **GPT-3 (175B)** on most benchmarks, and **LLaMA-65B** is competitive with the best models, **Chinchilla-70B** and **PaLM-540B**.

The focus of this work is to train a series of language models that achieve the best possible performance **at various inference budgets**, by **training on more tokens** than what is typically used.

这项工作的重点是训练一系列语言模型，通过训练比通常使用的 **更多的 Token**，**在各种推理预算下** 实现最佳性能。

### 方法

#### 1）预训练的数据

使用多种公开可用的数据集，并按照不同的比例（概率）进行混合和采样，如下图所示：


- English CommonCrawl（67%）

  - 使用 5 个 CommonCrawl dumps（从 2017 年到 2020 年）

  - 后处理：

    - 删除重复数据（行级别上）

    - 使用 fastText 线性分类器识别语言类别，删除非英语的数据

    - 使用 n-gram 语言模型过滤掉低质量的数据

  - 在实验中，作者观察到使用 CommonCrawl 数据集（经过不同的预处理）可以提高性能

- C4（15%）

  - 预处理的过程也包括去重（deduplication）和语言识别（language identity）

- Github（4.5%）

  - 在 Google BigQuery 中公开可用的 Github 数据集

our entire training dataset contains roughly **1.4T tokens after tokenization**. 对于大部分的训练数据，每个 Token 在训练期间仅使用一次（一个 epoch），除了 Wikipedia 和 Books domain，作者在这些领域执行了大约两个 epoch。



#### 2）模型结构 

使用 Transformer 架构，但是使用了下面的许多改进方法，这些方法来源于现有的 LLM 模型：

- Pre-normalization【GPT-3】

  - 相比原始的 Transformer 使用 Post-normalization，这里使用 Pre-normalization，
 
- 使用 **RMSNorm** 归一化函数

- SwiGLU【PaLM】

  - 使用 SwiGLU 激活函数替代 ReLU 激活函数，以便提高性能。使用 $$\frac{2}{3} 4d$$ 维度代替 PaLM 中的 $$4d$$ 维度。

- Rotary Embeddings（旋转位置编码）【GPTNeo】

  - 删除原有的绝对位置编码，使用 **旋转位置编码**

> - dog：0 * theta（旋转角度）
> 
> - The dog：1 * theta（旋转角度）
> 
> - The lucky dog：2 * theta（旋转角度）


#### 3）优化器

- 使用 AdamW 优化器，$$\beta_1=0.9$$、$$\beta_2=0.95$$

- We use a **cosine learning rate schedule**, such that the <u>final learning rate is equal to 10% of the maximal learning rate</u>.

- We use a weight decay of 0.1 and gradient clipping of 1.0.

- warmup

All models are trained with a batch size of 4M tokens.

#### 4）高效的实现

使用了许多优化方法来提高模型的训练速度：

- 使用了一种高效的 **causal multi-head attention** 实现

  - 在 xformers 库中

When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days. 在 2048 个 A100 GPU (80GB) 上，训练一个 epoch 大约需要 21 天。



### 主要的实验结果

与之前的工作一样，作者主要考虑模型的 Zero-shot 和 Few-shot 能力，并且在 20 个不同的 benchmarks 上进行实验。

- Zero-shot

  - 提供一个关于任务的文本描述（Prompt）和一个测试实例（test example）

  - 模型要么需要通过 open-ended 生成相应的答案（针对 free-form generation tasks）；要么需要对给定的选项进行排序，将排名最靠前的选项作为最终的答案（针对 multiple choice tasks）

- Few-shot

  - 提供一些（1 到 64 之间）任务实例和一个测试实例

  - 与 Zero-shot 一致，模型有两种可能的回答方式


#### 指令微调

作者通过简单的实验表明：使用指令微调能够迅速提高 LLaMA 模型在 MMLU（Massive Multitask Language Understanding）任务上的性能表现。这个模型作者将其称之为 LLaMA-I。


#### 安全性分析和碳排放

作者还对 LlaMA 模型进行了安全性分析，包括可能存在的偏见、毒性和错误信息。由于作者使用的数据中很大一部分来源于网络，因此模型有很大的概率会生成有害的、不正确的信息文本信息。

最后，作者也汇报了训练 LLaMA 所需要的碳排放。



## TODO 代码






## 参考

- LLaMA 论文：[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1)

- 模型下载

  - 知乎：

  - 博客：[LLaMA开源语言模型泄漏版下载](https://openai.wiki/llama-model-download.html)

  - CSDN：[Meta的LLama模型非官方下载方法](https://blog.csdn.net/u014297502/article/details/129829677)

- LLaMA "开源" 之后

  - 新智元：[开发者笑疯了！ LLaMa泄露引爆ChatGPT平替狂潮，开源LLM领域变天](https://zhuanlan.zhihu.com/p/620801077)


- bilibili：

  - 昇思：[第十五课：LLaMA](https://www.bilibili.com/video/BV1nN41157a9)

