---
layout: post
author: bookstall
tags: Prompt Tuning
categories: [Prompt Tuning]
excerpt: P-Tuning：
keywords: Prompt Tuning
title: P-Tuning
mathjax: true
sticky: true
---


在 finetuning 时增加了很多的数据（很多的指令 Instruction），然后再 unseen task 上进行 zero-shot inference

问题：
如何构造这么多的 Instruction？


OpenAI CLIP
- text token length：77
- visual token length：257（256 + 1）

R-Drop Loss

AutoModelForMaskedLM -> bert-base-chinese

AutoTokenizer -> bert-base-chinese

dataset -> datasets.load_dataset()

max_seq_len: max sequence length

max_label_len: max label (predicted value) length

p_tokens_len: p-tuning tokens length (numbers)